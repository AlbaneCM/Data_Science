{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing with Regular Expressions\n",
    "#### Discover Insights into Classic Texts\n",
    "\n",
    "Novels and text contain insights into ideologies and places that are often originally unknown to the reader. By reading a written piece, you uncover the opinions of the author on their chosen topic and come to understand both the topic and how the author thinks.\n",
    "\n",
    "In this project you will perform a natural language parsing analysis to gain deeper insight into one of two famous and often discussed novels in the public domain: Oscar Wilde’s The Picture of Dorian Gray or Homer’s The Iliad! Fear not if you haven’t heard or read the novels, one of the beauties of natural language parsing with regular expressions is the ability to gain insight into lengthy pieces of text without a formal read!\n",
    "\n",
    "By the end of this project, you will find out the main topics of discussion in the novel of your choosing and can begin to discern some of the author’s thoughts and beliefs!\n",
    "\n",
    "Project from Codecademy https://www.codecademy.com/paths/data-science/tracks/natural-language-processing-dsp/modules/parsing-with-regular-expressions-dsp/projects/nlp-regex-parsing-project <br>\n",
    "Adapted to work outside the Codecademy platform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing toolkits for Natural Language Processing --> NLTK is a leading platform for building Python programs to work with human language data www.nltk.org\n",
    "- Importing Counter --> To highlight the main topics of discussion in the novel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize \n",
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import collections\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing text of choice here\n",
    "text = open(\"the_iliad.txt\", encoding='utf-8').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence and word tokenizing text here\n",
    "\n",
    "sent_text = nltk.sent_tokenize(text) # this gives us a list of sentences\n",
    "\n",
    "# now looping over each sentence and tokenize it separately\n",
    "word_tokenized_text = []\n",
    "\n",
    "for sentence in sent_text:\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    word_tokenized_text.append(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'this', 'difficulty', 'attaches', 'itself', 'more', 'closely', 'to', 'an', 'age', 'in', 'which', 'progress', 'has', 'gained', 'a', 'strong', 'ascendency', 'over', 'prejudice', ',', 'and', 'in', 'which', 'persons', 'and', 'things', 'are', ',', 'day', 'by', 'day', ',', 'finding', 'their', 'real', 'level', ',', 'in', 'lieu', 'of', 'their', 'conventional', 'value', '.']\n"
     ]
    }
   ],
   "source": [
    "# storing and printing any word tokenized sentence here\n",
    "\n",
    "single_word_tokenized_sentence = word_tokenized_text[100]\n",
    "print(single_word_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a for loop through each word tokenized sentence here\n",
    "\n",
    "for word in word_tokenized_text:\n",
    "  # part-of-speech tagging each sentence and appending to list of pos-tagged sentences here\n",
    "    pos_tagged_result = pos_tag(word)\n",
    "    pos_tagged_text.append(pos_tagged_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 'CC'), ('this', 'DT'), ('difficulty', 'NN'), ('attaches', 'VBZ'), ('itself', 'PRP'), ('more', 'RBR'), ('closely', 'RB'), ('to', 'TO'), ('an', 'DT'), ('age', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('progress', 'NN'), ('has', 'VBZ'), ('gained', 'VBN'), ('a', 'DT'), ('strong', 'JJ'), ('ascendency', 'NN'), ('over', 'IN'), ('prejudice', 'NN'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('which', 'WDT'), ('persons', 'NNS'), ('and', 'CC'), ('things', 'NNS'), ('are', 'VBP'), (',', ','), ('day', 'NN'), ('by', 'IN'), ('day', 'NN'), (',', ','), ('finding', 'VBG'), ('their', 'PRP$'), ('real', 'JJ'), ('level', 'NN'), (',', ','), ('in', 'IN'), ('lieu', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('conventional', 'JJ'), ('value', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# storing and printing any part-of-speech tagged sentence here\n",
    "\n",
    "single_pos_sentence = pos_tagged_text[100]\n",
    "print(single_pos_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining noun phrase chunk grammar here\n",
    "chunk_grammar = \"NP: {<DT>? <JJ>* <NN>} \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating noun phrase RegexpParser object here\n",
    "np_chunk_parser = RegexpParser(chunk_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining verb phrase chunk grammar here\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating verb phrase RegexpParser object here\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
    "np_chunked_text = []\n",
    "vp_chunked_text = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  and/CC\n",
      "  (NP this/DT difficulty/NN)\n",
      "  attaches/VBZ\n",
      "  itself/PRP\n",
      "  more/RBR\n",
      "  closely/RB\n",
      "  to/TO\n",
      "  (NP an/DT age/NN)\n",
      "  in/IN\n",
      "  which/WDT\n",
      "  (NP progress/NN)\n",
      "  has/VBZ\n",
      "  gained/VBN\n",
      "  (NP a/DT strong/JJ ascendency/NN)\n",
      "  over/IN\n",
      "  (NP prejudice/NN)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  in/IN\n",
      "  which/WDT\n",
      "  persons/NNS\n",
      "  and/CC\n",
      "  things/NNS\n",
      "  are/VBP\n",
      "  ,/,\n",
      "  (NP day/NN)\n",
      "  by/IN\n",
      "  (NP day/NN)\n",
      "  ,/,\n",
      "  finding/VBG\n",
      "  their/PRP$\n",
      "  (NP real/JJ level/NN)\n",
      "  ,/,\n",
      "  in/IN\n",
      "  (NP lieu/NN)\n",
      "  of/IN\n",
      "  their/PRP$\n",
      "  (NP conventional/JJ value/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# creating a for loop through each pos-tagged sentence here\n",
    "for pos_tagged_sentence in pos_tagged_text:\n",
    "  # chunking each sentence and appending to lists here\n",
    "\n",
    "  # for noun phrase chunks first\n",
    "    np_chunked_text.append(np_chunk_parser.parse(pos_tagged_sentence))\n",
    "  # and verb phrase chunks also\n",
    "    vp_chunked_text.append(vp_chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "print(np_chunked_text[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the np_chunk_counter function\n",
    "\n",
    "def np_chunk_counter(list_of_trees):\n",
    "    subtree_list = []\n",
    "    for i in range(len(list_of_trees)):\n",
    "        for subtree in list_of_trees[i].subtrees():\n",
    "            if subtree.label() == \"NP\":\n",
    "                for i in range(len(subtree)):\n",
    "                    subtree_list.append(subtree[i])\n",
    "    print(Counter(subtree_list).most_common(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the vp_chunk_counter function\n",
    "\n",
    "def vp_chunk_counter(list_of_trees):\n",
    "    subtree_list = []\n",
    "    for i in range(len(list_of_trees)):\n",
    "        for subtree in list_of_trees[i].subtrees():\n",
    "            if subtree.label() == \"VP\":\n",
    "                for i in range(len(subtree)):\n",
    "                    subtree_list.append(subtree[i])\n",
    "    print(Counter(subtree_list).most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('the', 'DT'), 10044), (('a', 'DT'), 1770), (('thy', 'JJ'), 523), (('this', 'DT'), 432), (('hector', 'NN'), 417), (('war', 'NN'), 356), (('jove', 'NN'), 311), (('i', 'NN'), 288), (('son', 'NN'), 284), (('day', 'NN'), 269), (('god', 'NN'), 264), (('great', 'JJ'), 253), (('hand', 'NN'), 252), (('plain', 'NN'), 248), (('troy', 'NN'), 248), (('an', 'DT'), 232), (('no', 'DT'), 218), (('field', 'NN'), 207), (('fight', 'NN'), 204), (('vain', 'NN'), 201), (('chief', 'NN'), 199), (('some', 'DT'), 194), (('thou', 'NN'), 194), (('man', 'NN'), 191), (('force', 'NN'), 191), (('fate', 'NN'), 190), (('race', 'NN'), 187), (('ground', 'NN'), 184), (('rage', 'NN'), 184), (('trojan', 'NN'), 180)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# storing and printing the most common NP-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis for The Iliad\n",
    "\n",
    "Looking at most_common_np_chunks, you can identify characters of importance in the text such as hector and jove based on their frequency. Additionally a location of importance, troy, is mentioned often. A theme of war can also implied by its high frequency count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('the', 'DT'), 963), (('is', 'VBZ'), 269), (('was', 'VBD'), 212), (('i', 'NN'), 167), (('a', 'DT'), 133), (('hector', 'NN'), 95), (('be', 'VB'), 91), (('has', 'VBZ'), 86), (('had', 'VBD'), 75), (('this', 'DT'), 72), ((\"'d\", 'VBD'), 68), (('thy', 'JJ'), 47), (('hero', 'NN'), 45), ((\"'t\", 'NN'), 44), (('stood', 'VBD'), 43), (('were', 'VBD'), 40), (('gave', 'VBD'), 38), (('fell', 'VBD'), 37), (('not', 'RB'), 36), (('are', 'VBP'), 35), (('great', 'JJ'), 35), (('lies', 'VBZ'), 35), (('thou', 'NN'), 35), (('homer', 'NN'), 34), (('came', 'VBD'), 34), (('jove', 'NN'), 34), (('battle', 'NN'), 33), (('day', 'NN'), 31), (('flew', 'VBD'), 30), (('no', 'DT'), 29)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# storing and printing the most common VP-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis for The Iliad\n",
    "\n",
    "Looking at most_common_vp_chunks, you can see that verb phrases of the form you defined in your chunk grammar do not appear as often in The Iliad as noun phrases. This can indicate a different style of writing taken by the author that does not follow traditional grammatical style (i.e. poetry). Even when chunks are not found, their absence can give you insight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
